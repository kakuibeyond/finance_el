{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ner_for_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting jieba==0.42.1\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 11.9 MB/s eta 0:00:01�                  | 8.4 MB 11.9 MB/s eta 0:00:01B 11.9 MB/s eta 0:00:01�███████████▎| 18.8 MB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/admin/.local/lib/python3.6/site-packages (from -r requirement.txt (line 2)) (1.16.6)\n",
      "Collecting stanza==1.3.0\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/1a/66/8efe2b2358d9973b8427b4e9cf17ef344bbe38f0ea20a0ba129a62716fa0/stanza-1.3.0-py3-none-any.whl (432 kB)\n",
      "\u001b[K     |████████████████████████████████| 432 kB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.62.3\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 758 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/admin/.local/lib/python3.6/site-packages (from stanza==1.3.0->-r requirement.txt (line 3)) (2.25.0)\n",
      "Requirement already satisfied: six in /home/admin/.local/lib/python3.6/site-packages (from stanza==1.3.0->-r requirement.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/admin/.local/lib/python3.6/site-packages (from stanza==1.3.0->-r requirement.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: protobuf in /home/admin/.local/lib/python3.6/site-packages (from stanza==1.3.0->-r requirement.txt (line 3)) (3.14.0)\n",
      "Collecting emoji\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/83/db/beeea7a485754d0841935bb7b2ad22816b2a71d3472b5eca55dce83b5d6f/emoji-1.6.3.tar.gz (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 60.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/admin/.local/lib/python3.6/site-packages (from requests->stanza==1.3.0->-r requirement.txt (line 3)) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/admin/.local/lib/python3.6/site-packages (from requests->stanza==1.3.0->-r requirement.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/admin/.local/lib/python3.6/site-packages (from requests->stanza==1.3.0->-r requirement.txt (line 3)) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/admin/.local/lib/python3.6/site-packages (from requests->stanza==1.3.0->-r requirement.txt (line 3)) (2.10)\n",
      "Building wheels for collected packages: jieba, emoji\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=a97c6f4e8b48614d50bb020249654503cea2f2b6f392b42afd84eb88be971b4b\n",
      "  Stored in directory: /data/nas/workspace/jupyter/.cache/pip/wheels/f9/db/df/6da4d19a55a3b7e855f6406fad9f384d3edebdede4a02d09d8\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170281 sha256=f1128df72f07818fe7caafcc0daca53254f0ab8e8ed3bb2462ab4ce3bcb9d95c\n",
      "  Stored in directory: /data/nas/workspace/jupyter/.cache/pip/wheels/98/cb/d0/f86ea2013e996296c38a62a0f804b2aad23a88fe04c5fa7a2b\n",
      "Successfully built jieba emoji\n",
      "Installing collected packages: jieba, tqdm, emoji, stanza\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.53.0\n",
      "    Uninstalling tqdm-4.53.0:\n",
      "      Successfully uninstalled tqdm-4.53.0\n",
      "Successfully installed emoji-1.6.3 jieba-0.42.1 stanza-1.3.0 tqdm-4.62.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 27 19:15:26 2022\n",
      "[AMP INFO][Frontend.cpp:152][1643282126:378132]pid=745, start to allocate gpu resource ...\n",
      "+------------------------------------------------------------------------------+\n",
      "|    VGPU_SMI 450.80.02     DRIVER_VERSION: 450.80.02     CUDA Version: 10.2   |\n",
      "+-------------------------------------------+----------------------------------+\n",
      "| GPU  Name                Bus-Id           |        Memory-Usage     GPU-Util |\n",
      "|===========================================+==================================|\n",
      "|   0  Tesla V100-SXM2...  00000000:00:07.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 |     0MiB /  7531MiB    0% /   0% |\n",
      "+-------------------------------------------+----------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 19:16:00 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2022-01-27 19:17:35 INFO: Use device: gpu\n",
      "2022-01-27 19:17:35 INFO: Loading: tokenize\n",
      "2022-01-27 19:17:35 INFO: Loading: pos\n"
     ]
    }
   ],
   "source": [
    "zh_nlp = stanza.Pipeline('zh', processors='tokenize,ner,pos', \n",
    "                                tokenize_pretokenized=True,\n",
    "                                use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import jieba\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    filename='./log/ner.log',\n",
    "                    format=\"%(asctime)s - %(name)s - %(levelname)-9s - %(filename)-8s : %(lineno)s line - %(message)s\",\n",
    "                    # -8表示占位符，让输出左对齐，输出长度都为8位\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "# 文档列表，返回与文档列表等长的标签列表，句子无实体用'O'表示\n",
    "def ner(docs):\n",
    "    logging.info('=====len of doc:{}====='.format(len(docs)))\n",
    "    t1=time.time()\n",
    "    text_w=[jieba.lcut(t) for t in docs]\n",
    "    t2=time.time()\n",
    "    logging.debug('cut words:{:.2f}seconds'.format(t2-t1))\n",
    "    res=[]\n",
    "    keep_type=['GPE','LOC','PERSON','ORG']\n",
    "    # logging.info('===开始加载文档===')\n",
    "    doc = zh_nlp(text_w)\n",
    "    t3=time.time()\n",
    "    logging.debug('loading docs:{:.2f}seconds'.format(t3-t2))\n",
    "    # logging.info('===结束加载文档===')\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        # print(\"Sentence: \" + sent.text)  # 因为提前分词，所以这里文本（自带空格分割）和后面分词结果打印出来一模一样\n",
    "        # print(\"Tokenize：\" + '||'.join(token.text for token in sent.tokens))  # 中文分词\n",
    "        curr_en=set()# 当前句子实体集合\n",
    "        for ent in sent.ents:\n",
    "            if ent.type in keep_type:\n",
    "                t=ent.text.replace(' ','')\n",
    "                curr_en.add(f'{t}/{ent.type}')\n",
    "        if len(curr_en)>0:\n",
    "            res.append(';'.join(curr_en))\n",
    "        else:\n",
    "            res.append('O')\n",
    "    assert len(res)==len(docs)\n",
    "    logging.debug('select entitys:{:.2f}seconds'.format(time.time()-t3))\n",
    "    return res\n",
    "\n",
    "\n",
    "# 填充指定索引区间内的实体\n",
    "def ner_partation(df,start,end, entity_list, col='mentions'):\n",
    "    assert len(entity_list)==end-start+1\n",
    "    df.loc[start:end,col]=entity_list\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.869 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 21/21 [00:05<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r'data/segment_articles.csv',sep='\\t',index_col=0) # 省略的当前目录即pwd\n",
    "# max_line=df.shape[0]\n",
    "max_line=2050 # 先测试一下2050行/100,耗时3.71秒\n",
    "t_per_epoch=100\n",
    "epochs = max_line // t_per_epoch + 1 # 101轮循环完\n",
    "logging.info('NER START:lines:{},t_per_epoch:{},epochs:{}'.format(max_line,t_per_epoch,epochs))\n",
    "start_t=time.time()\n",
    "for i in tqdm(range(epochs)):\n",
    "    try:\n",
    "        logging.debug('current=====epoch:{}/{}====='.format(i+1,epochs))\n",
    "        start = i*t_per_epoch\n",
    "        end = min((i+1)*t_per_epoch,max_line)-1\n",
    "        texts=df.loc[start:end, 'text'].tolist()\n",
    "        entitys=ner(texts)\n",
    "        ner_partation(df,start,end, entitys, col='mentions')\n",
    "        if i%50==0:\n",
    "            logging.info('epoch {},temp save data'.format(i+1))\n",
    "            df.to_csv('segment_articles_tmp_end_{}.csv'.format(end),sep='\\t')\n",
    "    except Exception as e:\n",
    "        logging.error('{} error！！ current start:{}, 耗时:{:.2f}seconds'.format(e,start,time.time() - start_t))\n",
    "        df.to_csv('segment_articles_start{}.csv'.format(start),sep='\\t')\n",
    "logging.info(\"所有文档实体识别完成，总耗时: {:.2f}秒\".format(time.time() - start_t))\n",
    "df.to_csv('segment_articles_test0122.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
